{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code sets up a query engine (query_engine) using a VectorStoreIndex and a language model.\n",
    "\n",
    "The primary purpose of this code is to implement a Retrieval-Augmented Generation (RAG) system capable of answering questions based on the content of personal PDF documents. These documents are processed and converted into embeddings, indexed using LlamaIndex, and stored in a persistent vector database. The vector database (ChromaDB) was pre-populated in an earlier step, likely using the ‘run_first_prepare_data.ipynb’ notebook.\n",
    "\n",
    "This implementation leverages the OpenAI embedding model (OpenAIEmbedding) to encode the content into vector representations and the Groq language model to generate human-like responses. The vector database serves as the backbone of the RAG system, enabling efficient similarity searches and retrieval of relevant information to enhance the accuracy and context of the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from warnings import filterwarnings\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PDF_DIR = 'input/pdfs/'\n",
    "OUTPUT_MD_DIR = 'input/mds/'\n",
    "CHROMADB_DIR = 'database/vector_store/'\n",
    "CHROMADB_COLLECTION = 'rag_collection'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code then defines a query engine (query_engine) using the VectorStoreIndex and the language model.\n",
    "\n",
    "The purpose of this code is likely to set up a RAG system that can answer questions based on the content of personal PDF documents, which are processed and indexed using LlamaIndex, leveraging the vector database created in the previous step by the 'run_first_prepare_data.ipynb' notebook.\"\n",
    "\n",
    "This adds the information that the vector database created in the previous step is being used to power the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_embed_model = OpenAIEmbedding(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "llm_model = Groq(model=\"llama3-70b-8192\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path = CHROMADB_DIR)\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=CHROMADB_COLLECTION)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection = chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=chroma_embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine(llm = llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'YOUR FIRST QUESTION HERE?'\n",
    "result = query_engine.query(QUERY)\n",
    "ic(result.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rag-docling-llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
